{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action1_high = 2.97\n",
    "action1_low = -2.97\n",
    "action2_high = 1.57\n",
    "action2_low = -1.57\n",
    "action3_high = 1.57\n",
    "action3_low = -1.57\n",
    "action4_high = 0.04\n",
    "action4_low = -2.81\n",
    "action5_high = 1.51\n",
    "action5_low = -2.77\n",
    "action6_high = 1.57\n",
    "action6_low = -1.57\n",
    "action7_high = 2.97\n",
    "action7_low = -2.96\n",
    "\n",
    "minibatch_size = 128\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "gamma = 0.98\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_size, action_size, goal_size, kind):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.goal_size = goal_size\n",
    "        self.kind = kind\n",
    "        self.eparams = {}   #evaluation network parameter\n",
    "        self.taprams = {}\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.sess = tf.Session()\n",
    "            set_parameter(self.eparams)\n",
    "            set_parameter(self.tparams)\n",
    "            #initialize target network parameter\n",
    "            for p in self.eparams:\n",
    "                tf.assign(self.tparams[p], self.eparams[p])\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.action = self.build_network(self.eparams)\n",
    "            self.optimaizer = self.train_evaluation_network()\n",
    "\n",
    "    def set_parameter(self, params):\n",
    "        params['w1'] = tf.Variable(tf.truncated_normal(shape = [self.state_size + self.goal_size, 256], mean = 0.0, stddev = 1.0))\n",
    "        params['b1'] = tf.Variable(tf.zeros(256))\n",
    "        params['w2'] = tf.Variable(tf.truncated_normal(shape = [256, 64], mean = 0.0, stddev = 1.0))\n",
    "        params['b2'] = tf.Variable(tf.zeros(64))\n",
    "        params['w3'] = tf.Variable(tf.truncated_normal(shape = [64, self.action_size], mean = 0.0, stddev = 1.0))\n",
    "        params['b3'] = tf.Variable(tf.zeros(self.action_size))\n",
    "\n",
    "    def build_network(self, params):\n",
    "        state = tf.placeholder(tf.float32, [None, self.state_size])\n",
    "        goal = tf.placeholder(tf.float32, [None, self.goal_size])\n",
    "        inputs = tf.concat([state, goal], axis = 1)\n",
    "        o = tf.matmul(inputs, params['w1']) + params['b1']\n",
    "        y = tf.nn.relu(o)\n",
    "        o = tf.matmul(y, params['w2']) + params['b2']\n",
    "        y = tf.nn.relu(o)\n",
    "        o = tf.matmul(y, params['w3']) + params['b3']\n",
    "        y = tf.nn.tanh(o)\n",
    "\n",
    "        c1 = [(action1_high - action1_low) / 2,\n",
    "              (action2_high - action2_low) / 2,\n",
    "              (action3_high - action3_low) / 2,\n",
    "              (action4_high - action4_low) / 2,\n",
    "              (action5_high - action5_low) / 2,\n",
    "              (action6_high - action6_low) / 2,\n",
    "              (action7_high - action7_low) / 2]\n",
    "        c2 = [(action1_high + action1_low) / 2,\n",
    "              (action2_high + action2_low) / 2,\n",
    "              (action3_high + action3_low) / 2,\n",
    "              (action4_high + action4_low) / 2,\n",
    "              (action5_high + action5_low) / 2,\n",
    "              (action6_high + action6_low) / 2,\n",
    "              (action7_high + action7_low) / 2]\n",
    "        y = y * c1 + c2                                 #scaleout\n",
    "        return y\n",
    "\n",
    "    def train_evaluation_network(self):\n",
    "        dq_da = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "        eparams = []\n",
    "        key = []\n",
    "        for p in self.eparams:\n",
    "            eparams.append(self.eparams[p])\n",
    "        grads = tf.gradients(self.action, eparams, dq_da)\n",
    "        grads = np.array(grads)\n",
    "        grad = -1 * np.sum(grads, axis = 0) / minibatch_size\n",
    "        grad = grad.tolist()\n",
    "        optimizer = tf.train.AdamOptimizer(actor_learning_rate).apply_gradients(zip(grad, eparams))\n",
    "        for i, p in zip(6, self.eparams):\n",
    "            tf.assign(self.eparams[p], eparams[i])\n",
    "        return optimizer\n",
    "\n",
    "    def update_target_network(self):\n",
    "        for p in self.eparams:\n",
    "            tf.assign(self.tparams[p], tau * self.eparams[p] + (1 - tau) * self.tparams[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_size, action_size, goal_size, kind):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.goal_size = goal_size\n",
    "        self.kind = kind\n",
    "        self.eparams = {}   #evaluation network parameter\n",
    "        self.tparams = {}   #target network parameter\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.sess = tf.Session()\n",
    "            #initialize evaluation network parameter\n",
    "            set_parameter(self.eparams)\n",
    "            set_parameter(self.tparams)\n",
    "            #initialize target network parameter\n",
    "            for p in self.eparams:\n",
    "                tf.assign(self.tparams[p], self.eparams[p])\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.q = self.build_network(self.kind)\n",
    "            self.optimaizer = self.train_evaluation_network()\n",
    "            self.qgrad = self.dq_da()\n",
    "\n",
    "    def set_parameter(self, params):\n",
    "        params['w1'] = tf.Variable(tf.truncated_normal(shape = [self.state_size + self.goal_size, 256], mean = 0.0, stddev = 1.0))\n",
    "        params['b1'] = tf.Variable(tf.zeros(256))\n",
    "        params['w2'] = tf.Variable(tf.truncated_normal(shape = [256, 64], mean = 0.0, stddev = 1.0))\n",
    "        params['b2'] = tf.Variable(tf.zeros(64))\n",
    "        params['w3'] = tf.Variable(tf.truncated_normal(shape = [64, self.action_size], mean = 0.0, stddev = 1.0))\n",
    "        params['b3'] = tf.Variable(tf.zeros(self.action_size))\n",
    "\n",
    "    def build_network(self, params):\n",
    "        state = tf.placeholder(tf.float32, [None, self.state_size])\n",
    "        action = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "        goal = tf.placeholder(tf.float32, [None, self.goal_size])\n",
    "        inputs = tf.concat([state, action, goal], axis = 1)\n",
    "        o = tf.matmul(inputs, params['w1']) + params['b1']\n",
    "        y = tf.nn.relu(o)\n",
    "        o = tf.matmul(y, params['w2']) + params['b2']\n",
    "        y = tf.nn.relu(o)\n",
    "        y = tf.matmul(y, params['w3']) + params['b3']\n",
    "        return y\n",
    "\n",
    "    def train_evaluation_network(self):\n",
    "        q_ = tf.placeholder(tf.float32, [None, 1])  #next state q\n",
    "        reward = tf.placeholder(tf.float32, [None, 1])\n",
    "        done = tf.placeholder(tf.float32, [None, 1])   #if episode end 1, otherwise 0\n",
    "        q_target = reward + (1 - done) * gamma * q_\n",
    "        eparams = []\n",
    "        for p in self.eparams:\n",
    "            eparams.append(self.eparams[p])\n",
    "        loss = tf.losses.mean_squared_error(q_target, self.q)\n",
    "        grad = tf.gradients(loss, eparams)\n",
    "        optimizer = tf.train.AdamOptimizer(critic_learning_rate).apply_gradients(zip(grad, eparams))\n",
    "        for i, p in zip(6, self.eparams):\n",
    "            tf.assign(self.eparams[p], eparams[i])\n",
    "        return optimizer\n",
    "\n",
    "    def dq_da(self):\n",
    "        action = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "        grad = tf.gradients(self.q, action)\n",
    "        return grad\n",
    "\n",
    "    def update_target_network(self):\n",
    "        for p in self.eparams:\n",
    "            tf.assign(self.tparams[p], tau * self.eparams[p] + (1 - tau) * self.tparams[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.40066016, -0.6626734 ],\n",
      "       [-0.28813574,  1.6087884 ],\n",
      "       [-1.658034  ,  0.09471382],\n",
      "       [ 0.11548227, -1.1697099 ]], dtype=float32), array([[-0.14110884, -1.0847067 ],\n",
      "       [-0.3824489 ,  0.7458162 ]], dtype=float32), array([0., 0.], dtype=float32), array([0., 0.], dtype=float32), array([0., 0.], dtype=float32), array([[-1.0118456 , -0.16460407],\n",
      "       [-0.17729041,  0.45414558]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "eparamslist = actor.grad()\n",
    "print actor.sess.run(eparamslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
